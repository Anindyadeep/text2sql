# Roadmap

1. Start with making the latest changes in BIRDBench with new API
2. And make the dataset for the same 
3. Cleanup WikiSQL for hault now (since it can be done later)
4. Make the HF dataset for BIRD and write fine-tuning scripts
5. Then start with Gretel AI with the same format (but only for training). This will be done for an another run. 


WikiSQL Arc:

1. First clean things up so that there is no descripepancy in the data strucutre. 
2. Then use gpt-4o / mini to correct the human_readible SQL Statement
3. And then publish the dataset into huggingface
4. Then write the fine-tuning script for the same.


PYTHONPATH=. torchrun --master_port=1211 ./tests/train_v1.py \
    --model_name_or_path google/gemma-2-2b-it \
    --bf16 True \
    --output_dir ./output_gemma_v0.0.1/ \
    --num_train_epochs 1 \
    --per_device_train_batch_size 4 \
    --per_device_eval_batch_size 4 \
    --gradient_accumulation_steps 8 \
    --gradient_checkpointing True \
    --evaluation_strategy "no" \
    --learning_rate 1e-5 \
    --weight_decay 0.1 \
    --lr_scheduler_type "cosine" \
    --warmup_steps 0 \
    --tf32 True \
    --logging_steps 1 \
    --save_strategy "steps" \
    --save_steps 500 \
    --save_total_limit 1 
