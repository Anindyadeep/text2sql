{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/anindya/Submission/text2sql/text2sql\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "premsql datasets helps to use different already available and pre-processed datasets in a simple way. Since Text-to-SQL is a complex task and requires data which has a depdenency of database and tables. \n",
    "\n",
    "premsql datasets provides simple APIs to use those and also helps you to create your own dataset using your own private databases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently the following datasets are readily available:\n",
    "\n",
    "1. [BirdBench Dataset](https://huggingface.co/datasets/premai-io/birdbench)\n",
    "2. [Spider Unified Datasets](https://huggingface.co/datasets/premai-io/spider)\n",
    "3. [Domains](https://huggingface.co/datasets/premai-io/domains)\n",
    "4. [Gretel AI Dataset](https://huggingface.co/datasets/gretelai/synthetic_text_to_sql) (A synthetic text to SQL dataset by Gretel AI)\n",
    "\n",
    "Now we are going to see how to use these datasets in a simple way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/deep/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-09-09 04:26:34,697 - [BIRD-DATASET] - INFO - Loaded Bird Dataset\n"
     ]
    }
   ],
   "source": [
    "from premsql.datasets import Text2SQLDataset\n",
    "from premsql.utils import print_data\n",
    "# load the bird dataset\n",
    "\n",
    "bird_dataset = Text2SQLDataset(\n",
    "    dataset_name='bird', split=\"train\", force_download=False,\n",
    "    dataset_folder=\"/root/anindya/text2sql/data\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, this is just the object which has the raw the data. This object consist of two methods: \n",
    "\n",
    "1. `raw_dataset`: This will return a dict containing the raw data opened form the json file. \n",
    "2. `filters_available`: This will return the list of filters available for the dataset.\n",
    "\n",
    "So for our train dataset here is how we can see the raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'db_id': 'movie_platform',\n",
       " 'question': 'Name movie titles released in year 1945. Sort the listing by the descending order of movie popularity.',\n",
       " 'evidence': 'released in the year 1945 refers to movie_release_year = 1945;',\n",
       " 'SQL': 'SELECT movie_title FROM movies WHERE movie_release_year = 1945 ORDER BY movie_popularity DESC LIMIT 1'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_bird_training_dataset = bird_dataset.raw_dataset\n",
    "raw_bird_training_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can also see what all filters are available for the dataset. You can simply use `.filters_available` to see the available filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['db_id']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bird_dataset.filter_availables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in order to load the processed dataset, you can simply call `setup_dataset` method. This will load the processed dataset and return the dataset object. \n",
    "\n",
    "This dataset has certain (optional) methods available for furthur customization:\n",
    "\n",
    "- filter_by: tuple | None: This will filter the dataset based on the given filter.\n",
    "\n",
    "- num_rows: int | None: This will return the number of rows from the dataset.\n",
    "\n",
    "- num_fewshot: int | None: This will determine how many few shot examples to create in the prompt\n",
    "\n",
    "- model_name_or_path: str | None: This will apply the prompt template of the model you choose. For example, if you want to finetune a llama model then it will wrap the prompt with the llama model prompt template.\n",
    "\n",
    "Also if this is not provided then it will not tokenize the dataset. \n",
    "\n",
    "- prompt_template: str | None: If you want to use any other kind of prompt template then you can provide that here. You can check out the default prompt template [here](/premsql/datasets/prompts.py). \n",
    "\n",
    "**Note**:\n",
    "If `model_name_or_path` is provided then it will automatically use the prompt template of that model and tokenize, otherwise it will not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-09 04:26:49,099 - [BIRD-DATASET] - INFO - Setting up Bird Dataset\n",
      "Applying prompt: 100%|██████████| 3/3 [00:00<00:00, 1865.52it/s]\n",
      "2024-09-09 04:26:49,509 - [DATASET] - INFO - Casted dataset with model chat template\n",
      "2024-09-09 04:26:49,510 - [DATASET] - INFO - Starting Tokenization ...\n",
      "Tokenizing: 100%|██████████| 3/3 [00:00<00:00, 105.07it/s]\n",
      "Tokenizing: 100%|██████████| 3/3 [00:00<00:00, 179.29it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([32013, 32013,  2042,  ...,   207,    16, 32021]),\n",
       " 'labels': tensor([ -100,  -100,  -100,  ...,   207,    16, 32021]),\n",
       " 'raw': {'db_id': 'movie_platform',\n",
       "  'question': 'Name movie titles released in year 1945. Sort the listing by the descending order of movie popularity.',\n",
       "  'evidence': 'released in the year 1945 refers to movie_release_year = 1945;',\n",
       "  'SQL': 'SELECT movie_title FROM movies WHERE movie_release_year = 1945 ORDER BY movie_popularity DESC LIMIT 1',\n",
       "  'db_path': '/root/anindya/text2sql/data/bird/train/train_databases/movie_platform/movie_platform.sqlite',\n",
       "  'prompt': '<｜begin▁of▁sentence｜>You are an AI programming assistant, utilizing the Deepseek Coder model, develo....tles released in year 1945. Sort the listing by the descending order of movie popularity.\\n\\n# SQL: \\n\\n'}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's setup the bird dataset \n",
    "\n",
    "bird_dataset = bird_dataset.setup_dataset(\n",
    "    model_name_or_path=\"premai-io/prem-1B-SQL\", \n",
    "    num_fewshot=3, \n",
    "    num_rows=3\n",
    ")\n",
    "\n",
    "print_data(bird_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes tokenization could be time consuming, and it could be computation heavt. So, you can also preview the dataset without even tokenizing first. Here is\n",
    "how you do it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-09 04:27:12,344 - [BIRD-DATASET] - INFO - Loaded Bird Dataset\n",
      "2024-09-09 04:27:12,345 - [BIRD-DATASET] - INFO - Setting up Bird Dataset\n",
      "Applying prompt: 100%|██████████| 3/3 [00:00<00:00, 1908.24it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'db_id': 'movie_platform',\n",
       " 'question': 'Name movie titles released in year 1945. Sort the listing by the descending order of movie popularity.',\n",
       " 'evidence': 'released in the year 1945 refers to movie_release_year = 1945;',\n",
       " 'SQL': 'SELECT movie_title FROM movies WHERE movie_release_year = 1945 ORDER BY movie_popularity DESC LIMIT 1',\n",
       " 'db_path': '/root/anindya/text2sql/data/bird/train/train_databases/movie_platform/movie_platform.sqlite',\n",
       " 'prompt': '\\n# Follow these instruction:\\nYou will be given schemas of tables of a database. Your job is to write....itles released in year 1945. Sort the listing by the descending order of movie popularity.\\n\\n# SQL: \\n'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bird_dataset_without_tokenization = Text2SQLDataset(\n",
    "    dataset_name='bird', split=\"train\", force_download=False,\n",
    "    dataset_folder=\"/root/anindya/text2sql/data\"\n",
    ").setup_dataset(\n",
    "    model_name_or_path=None, num_fewshot=3, num_rows=3\n",
    ")\n",
    "\n",
    "print_data(bird_dataset_without_tokenization[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BirdDataset has two instance, a `train` and `validation` instance. For train dataset, you can only filter by `db_id`. This will only return results which are belonging to that database id. \n",
    "\n",
    "For BirdDevDataset you can filter by `db_id` and `difficulty`. Here is how you load a validation dataset and then filter by `difficulty`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-09 04:27:20,270 - [BIRD-DATASET] - INFO - Loaded Bird Dataset\n",
      "2024-09-09 04:27:20,271 - [BIRD-DATASET] - INFO - Setting up Bird Dataset\n",
      "Applying prompt: 100%|██████████| 100/100 [00:00<00:00, 2101.37it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the BirdBench dev dataset and filter the dataset by \n",
    "# difficulty\n",
    "\n",
    "bird_validation = Text2SQLDataset(\n",
    "    dataset_name='bird', split=\"validation\", force_download=False,\n",
    "    dataset_folder=\"/root/anindya/text2sql/data\"\n",
    ").setup_dataset(\n",
    "    model_name_or_path=None, \n",
    "    num_fewshot=3, \n",
    "    num_rows=100,\n",
    "    filter_by=(\"difficulty\", \"simple\")\n",
    ")\n",
    "\n",
    "# count the number of examples in the dataset which has \n",
    "# difficulty level as simple\n",
    "\n",
    "len([\n",
    "    example for example in bird_validation \n",
    "    if example[\"difficulty\"] == \"simple\"\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly we can also filter by the dataset by `db_id`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-09 04:27:28,490 - [BIRD-DATASET] - INFO - Loaded Bird Dataset\n",
      "2024-09-09 04:27:28,491 - [BIRD-DATASET] - INFO - Setting up Bird Dataset\n",
      "Applying prompt: 100%|██████████| 1534/1534 [00:00<00:00, 2537.01it/s]\n",
      "2024-09-09 04:27:29,414 - [DATASET] - INFO - Casted dataset with model chat template\n",
      "2024-09-09 04:27:29,415 - [DATASET] - INFO - Starting Tokenization ...\n",
      "Tokenizing: 100%|██████████| 1534/1534 [00:09<00:00, 161.93it/s]\n",
      "Tokenizing: 100%|██████████| 1534/1534 [00:09<00:00, 161.71it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([32013, 32013,  2042,  ...,   207,    16, 32021]),\n",
       " 'labels': tensor([ -100,  -100,  -100,  ...,   207,    16, 32021]),\n",
       " 'raw': {'question_id': 0,\n",
       "  'db_id': 'california_schools',\n",
       "  'question': 'What is the highest eligible free rate for K-12 students in the schools in Alameda County?',\n",
       "  'evidence': 'Eligible free rate for K-12 = `Free Meal Count (K-12)` / `Enrollment (K-12)`',\n",
       "  'SQL': \"SELECT `Free Meal Count (K-12)` / `Enrollment (K-12)` FROM frpm WHERE `County Name` = 'Alameda' ORDER BY (CAST(`Free Meal Count (K-12)` AS REAL) / `Enrollment (K-12)`) DESC LIMIT 1\",\n",
       "  'difficulty': 'simple',\n",
       "  'db_path': '/root/anindya/text2sql/data/bird/validation/dev_databases/california_schools/california_schools.sqlite',\n",
       "  'prompt': '<｜begin▁of▁sentence｜>You are an AI programming assistant, utilizing the Deepseek Coder model, develo....hat is the highest eligible free rate for K-12 students in the schools in Alameda County?\\n\\n# SQL: \\n\\n'}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bird_validation = Text2SQLDataset(\n",
    "    dataset_name='bird', split=\"validation\", force_download=False,\n",
    "    dataset_folder=\"/root/anindya/text2sql/data\"\n",
    ").setup_dataset(\n",
    "    model_name_or_path=\"premai-io/prem-1B-SQL\",\n",
    ")\n",
    "print_data(bird_validation[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it, thats how easy it is to use the datasets. Similarly you can also use other available datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 401 files: 100%|██████████| 401/401 [00:17<00:00, 22.75it/s]\n",
      "2024-09-09 04:28:35,739 - [SPIDER-DATASET] - INFO - Loaded Spider Dataset\n",
      "2024-09-09 04:28:35,744 - [SPIDER-DATASET] - INFO - Setting up Spider Dataset\n",
      "Applying prompt: 100%|██████████| 3/3 [00:00<00:00, 1572.67it/s]\n",
      "2024-09-09 04:28:36,088 - [DATASET] - INFO - Casted dataset with model chat template\n",
      "2024-09-09 04:28:36,089 - [DATASET] - INFO - Starting Tokenization ...\n",
      "Tokenizing: 100%|██████████| 3/3 [00:00<00:00, 248.45it/s]\n",
      "Tokenizing: 100%|██████████| 3/3 [00:00<00:00, 276.99it/s]\n"
     ]
    }
   ],
   "source": [
    "# Loading Spider Dataset\n",
    "\n",
    "spider_dataset = Text2SQLDataset(\n",
    "    dataset_name=\"spider\",\n",
    "    split=\"train\",\n",
    "    dataset_folder=\"../data\",\n",
    ").setup_dataset(\n",
    "    num_fewshot=3,\n",
    "    num_rows=3,\n",
    "    model_name_or_path=\"premai-io/prem-1B-SQL\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 6 files: 100%|██████████| 6/6 [00:05<00:00,  1.03it/s]\n",
      "2024-09-09 04:38:55,864 - [DOMAINS-DATASET] - INFO - Loaded Domains Dataset\n",
      "2024-09-09 04:38:55,867 - [DOMAINS-DATASET] - INFO - Setting up Domains Dataset\n",
      "Applying prompt: 100%|██████████| 3/3 [00:00<00:00, 1377.59it/s]\n",
      "2024-09-09 04:38:56,437 - [DATASET] - INFO - Casted dataset with model chat template\n",
      "2024-09-09 04:38:56,438 - [DATASET] - INFO - Starting Tokenization ...\n",
      "Tokenizing: 100%|██████████| 3/3 [00:00<00:00, 145.70it/s]\n",
      "Tokenizing: 100%|██████████| 3/3 [00:00<00:00, 160.63it/s]\n"
     ]
    }
   ],
   "source": [
    "## Loading Domains dataset\n",
    "\n",
    "domains = Text2SQLDataset(\n",
    "    dataset_name=\"domains\",\n",
    "    split=\"train\",\n",
    "    dataset_folder=\"../data\",\n",
    ").setup_dataset(\n",
    "    num_fewshot=3,\n",
    "    num_rows=3,\n",
    "    model_name_or_path=\"premai-io/prem-1B-SQL\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-09 04:38:45,602 - [UTILS] - INFO - Saved JSON in: ../data/gretel/train.json\n",
      "Applying prompt: 100%|██████████| 3/3 [00:00<00:00, 1909.97it/s]\n",
      "2024-09-09 04:38:46,543 - [DATASET] - INFO - Casted dataset with model chat template\n",
      "2024-09-09 04:38:46,543 - [DATASET] - INFO - Starting Tokenization ...\n",
      "Tokenizing: 100%|██████████| 3/3 [00:00<00:00, 326.80it/s]\n",
      "Tokenizing: 100%|██████████| 3/3 [00:00<00:00, 400.61it/s]\n"
     ]
    }
   ],
   "source": [
    "# Loading Gretel AI Dataset (This is a synthetic dataset)\n",
    "\n",
    "gretel_dataset = Text2SQLDataset(\n",
    "    dataset_name=\"gretel\",\n",
    "    split=\"train\",\n",
    "    dataset_folder=\"../data\",\n",
    ").setup_dataset(\n",
    "    num_fewshot=3,\n",
    "    num_rows=3,\n",
    "    model_name_or_path=\"premai-io/prem-1B-SQL\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 5097,\n",
       " 'question': 'What is the total volume of timber sold by each salesperson, sorted by salesperson?',\n",
       " 'schema': \"CREATE TABLE salesperson (salesperson_id INT, name TEXT, region TEXT); INSERT INTO salesperson (salesperson_id, name, region) VALUES (1, 'John Doe', 'North'), (2, 'Jane Smith', 'South'); CREATE TABLE timber_sales (sales_id INT, salesperson_id INT, volume REAL, sale_date DATE); INSERT INTO timber_sales (sales_id, salesperson_id, volume, sale_date) VALUES (1, 1, 120, '2021-01-01'), (2, 1, 150, '2021-02-01'), (3, 2, 180, '2021-01-01');\",\n",
       " 'SQL': 'SELECT salesperson_id, name, SUM(volume) as total_volume FROM timber_sales JOIN salesperson ON timber_sales.salesperson_id = salesperson.salesperson_id GROUP BY salesperson_id, name ORDER BY total_volume DESC;',\n",
       " 'context': \"CREATE TABLE salesperson (salesperson_id INT, name TEXT, region TEXT); INSERT INTO salesperson (salesperson_id, name, region) VALUES (1, 'John Doe', 'North'), (2, 'Jane Smith', 'South'); CREATE TABLE timber_sales (sales_id INT, salesperson_id INT, volume REAL, sale_date DATE); INSERT INTO timber_sales (sales_id, salesperson_id, volume, sale_date) VALUES (1, 1, 120, '2021-01-01'), (2, 1, 150, '2021-02-01'), (3, 2, 180, '2021-01-01');\",\n",
       " 'task_type': 'analytics and reporting',\n",
       " 'complexity': 'single join',\n",
       " 'db_id': 'forestry',\n",
       " 'db_path': None,\n",
       " 'prompt': '<｜begin▁of▁sentence｜>You are an AI programming assistant, utilizing the Deepseek Coder model, develo....tion: What is the total volume of timber sold by each salesperson, sorted by salesperson?\\n\\n# SQL: \\n\\n'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_data(gretel_dataset[0][\"raw\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the best things of the premsql datasets is that it supports packing. This means you can pack multiple datasets together and use them as a single dataset. This is very useful when you want to train on multiple datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of bird dataset: 3\n",
      "Length of spider dataset: 3\n",
      "Length of domains dataset: 3\n",
      "Length of gretel dataset: 3\n",
      "Length of merged dataset: 12\n"
     ]
    }
   ],
   "source": [
    "# Merge all the datasets\n",
    "\n",
    "print(f\"Length of bird dataset: {len(bird_dataset)}\")\n",
    "print(f\"Length of spider dataset: {len(spider_dataset)}\")\n",
    "print(f\"Length of domains dataset: {len(domains)}\")\n",
    "print(f\"Length of gretel dataset: {len(gretel_dataset)}\")\n",
    "\n",
    "merged_dataset = [*bird_dataset, *spider_dataset, *domains, *gretel_dataset]\n",
    "print(f\"Length of merged dataset: {len(merged_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([32013, 32013,  2042,  ...,   207,    16, 32021]),\n",
       " 'labels': tensor([ -100,  -100,  -100,  ...,   207,    16, 32021]),\n",
       " 'raw': {'db_id': 'movie_platform',\n",
       "  'question': 'Name movie titles released in year 1945. Sort the listing by the descending order of movie popularity.',\n",
       "  'evidence': 'released in the year 1945 refers to movie_release_year = 1945;',\n",
       "  'SQL': 'SELECT movie_title FROM movies WHERE movie_release_year = 1945 ORDER BY movie_popularity DESC LIMIT 1',\n",
       "  'db_path': '/root/anindya/text2sql/data/bird/train/train_databases/movie_platform/movie_platform.sqlite',\n",
       "  'prompt': '<｜begin▁of▁sentence｜>You are an AI programming assistant, utilizing the Deepseek Coder model, develo....tles released in year 1945. Sort the listing by the descending order of movie popularity.\\n\\n# SQL: \\n\\n'}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_data(merged_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does a prompt looks like in premsql\n",
    "\n",
    "You might wonder how does a prompt looks like in premsql. This is how a single prompt looks like when wrapped around a model's prompt template. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜>You are an AI programming assistant, utilizing the Deepseek Coder model, developed by Deepseek Company, and you only answer questions related to computer science. For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer\n",
      "### Instruction:\n",
      "\n",
      "# Follow these instruction:\n",
      "You will be given schemas of tables of a database. Your job is to write correct\n",
      "error free SQL query based on the question asked. Please make sure:\n",
      "\n",
      "1. Do not add ``` at start / end of the query. It should be a single line query in a  single line (string format)\n",
      "2. Make sure the column names are correct and exists in the table\n",
      "3. For column names which has a space with it, make sure you have put `` in that column name\n",
      "4. Think step by step and always check schema and question and the column names before writing the\n",
      "query. \n",
      "\n",
      "# Database and Table Schema:\n",
      "CREATE TABLE salesperson (salesperson_id INT, name TEXT, region TEXT); INSERT INTO salesperson (salesperson_id, name, region) VALUES (1, 'John Doe', 'North'), (2, 'Jane Smith', 'South'); CREATE TABLE timber_sales (sales_id INT, salesperson_id INT, volume REAL, sale_date DATE); INSERT INTO timber_sales (sales_id, salesperson_id, volume, sale_date) VALUES (1, 1, 120, '2021-01-01'), (2, 1, 150, '2021-02-01'), (3, 2, 180, '2021-01-01');\n",
      "\n",
      "\n",
      "\n",
      "# Here are some Examples on how to generate SQL statements and use column names:\n",
      "\n",
      "Question: What is the total volume of timber sold by each salesperson, sorted by salesperson?\n",
      "SQL: SELECT salesperson_id, name, SUM(volume) as total_volume FROM timber_sales JOIN salesperson ON timber_sales.salesperson_id = salesperson.salesperson_id GROUP BY salesperson_id, name ORDER BY total_volume DESC;\n",
      "\n",
      "\n",
      "# Question: What is the total volume of timber sold by each salesperson, sorted by salesperson?\n",
      "\n",
      "# SQL: \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(gretel_dataset[0][\"raw\"][\"prompt\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating your own dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we are going to see how we can make our own dataset similar like the above. Creating your own dataset could come with several customization and variables. One of the easiest ways to create your own dataset is to simply annotate the dataset in the given file structure:\n",
    "\n",
    "```\n",
    "├── databases\n",
    "│   ├── california_schools\n",
    "│       ├── california_schools.sqlite\n",
    "│   ├── card_games\n",
    "│   ├── codebase_community\n",
    "│   ├── debit_card_specializing\n",
    "│   ├── european_football_2\n",
    "│   ├── financial\n",
    "│   ├── formula_1\n",
    "│   ├── student_club\n",
    "│   ├── superhero\n",
    "│   ├── thrombosis_prediction\n",
    "│   └── toxicology\n",
    "├── train.json  \n",
    "├── validation.json # Optional \n",
    "```\n",
    "\n",
    "The reason we do this hierchy is, in a real world scenerio, we can have \n",
    "multiple databases, and each databases could be multiple tables. So this is how we organize them.\n",
    "\n",
    "Suppose you are saving everything inside `./data` folder then inside that folder you should have a `databases` folder (you can name it something else too) and a `train/validation.json` file. \n",
    "\n",
    "Inside the databases folder you should have multple sub folders where under each sub-folder you should have a `.sqlite` file of the same name. For example: if the db name is `california_schools` then you should have a .sqlite file inside `california_schools` folder. \n",
    "\n",
    "The `train` or `validation` JSON file, should be a list of dictionaries, having the following (required) keys:\n",
    "\n",
    "1. `db_id`: this represent the folder and the `.sqlite` file name.\n",
    "2. `question`: this represent the question asked by the user.\n",
    "3. `SQL`: This is the ground truth SQL.\n",
    "\n",
    "**Please note:** All the keys are case sensitive. Here is an example of a single datapoint. \n",
    "\n",
    "```json\n",
    "\"question_id\": 0,\n",
    "\"db_id\": \"california_schools\",\n",
    "\"question\": \"What is the highest eligible free rate for K-12 students in the schools in Alameda County?\",\n",
    "\"evidence\": \"Eligible free rate for K-12 = `Free Meal Count (K-12)` / `Enrollment (K-12)`\",\n",
    "\"SQL\": \"SELECT `Free Meal Count (K-12)` / `Enrollment (K-12)` FROM frpm WHERE `County Name` = 'Alameda' ORDER BY (CAST(`Free Meal Count (K-12)` AS REAL) / `Enrollment (K-12)`) DESC LIMIT 1\",\n",
    "\"difficulty\": \"simple\"\n",
    "```\n",
    "\n",
    "You can also keep other keys too, those will be automatically used as filter keys. Now you can use the code to automatically load your dataset from the folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from premsql.datasets import StandardDataset\n",
    "\n",
    "path = \"../data/bird/validation\"\n",
    "dataset = StandardDataset(\n",
    "    split=\"validation\",\n",
    "    dataset_path=path,\n",
    "    database_folder_name=\"dev_databases\",\n",
    "    json_file_name=\"validation.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[32m'db_id'\u001b[0m, \u001b[32m'difficulty'\u001b[0m\u001b[1m]\u001b[0m"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.filter_availables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have loaded our Bird dev database but this time we have used the `StandardDataset` class. A `StandardDataset` class acts like a template for all text2sql compatible datasets when following the above structure. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Towards more customization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last but not the least, there is one more level of customization that you can do while creating text-to-sql datasets. Till now all of these use cases shown above were tightly coupled with `.sqlite` specific databases. However if you:\n",
    "\n",
    "1. have different databases (like postgres or any cloud DB instance)\n",
    "2. or want to have lot of custom logics, before making prompts\n",
    "3. or add more utility on top of premsql\n",
    "\n",
    "This section will help you to achieve that. \n",
    "\n",
    "**Note** In case of the point number one, you can also migrate one subset of the dataset to SQLite. Once you have migrated a subset of your database content to SQLite and have done annotations for that, you can then go for the first route to create a Text2SQL compatible dataset for fine-tuning and inference. \n",
    "\n",
    "If you still want to go for full customization then you can achieve this with three steps. A detailed tutorial on this will be coming on future versions. However in short, you need to define two things for making a premsql fully custom dataset.\n",
    "\n",
    "**DatasetInstance:** A dataset instance helps to operations on individual datapoints. You need to extend `premsql.datasets.base.Text2SQLBaseInstance` class to define your own. Here is how a blueprint looks like:\n",
    "\n",
    "```python\n",
    "\n",
    "class CustomDataInstance(Text2SQLBaseInstance):\n",
    "    def __init__(self, dataset: list[dict]) -> None:\n",
    "        super().__init__(dataset=dataset)\n",
    "\n",
    "    def schema_prompt(self, db_path: str) -> str:\n",
    "        # write your schema prompt here\n",
    "        # you need to fetch the schema from your database\n",
    "        # and format it. For sqlite database it would look\n",
    "        # like this: SELECT sql FROM sqlite_master WHERE type='table' AND name='{table_name}\n",
    "        # check out Text2SQLBaseInstance premsql/datasets/base for more details\n",
    "```\n",
    "\n",
    "Additionally this class some more methods: `additional_prompt` `apply_prompt` those have some db agnostic default implementation, however you can change those too if you want. \n",
    "\n",
    "Once you have your instance defined, you can now define your custom class by inheriting from\n",
    "`premsql.datasets.base.Text2SQLBaseDataset` class, like this:\n",
    "\n",
    "\n",
    "```python\n",
    "class CustomText2SQLDataset(Text2SQLBaseDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        split: str,\n",
    "        dataset_folder: Optional[Union[str, Path]] = \"./data\",\n",
    "        hf_token: Optional[str] = None,\n",
    "        force_download: Optional[bool] = False,\n",
    "    ):\n",
    "        # Define your logic here\n",
    "        pass \n",
    "\n",
    "    def setup_dataset(\n",
    "        self,\n",
    "        filter_by: tuple | None = None,\n",
    "        num_rows: int | None = None,\n",
    "        num_fewshot: int | None = None,\n",
    "        model_name_or_path: str | None = None,\n",
    "        prompt_template: str | None = None,\n",
    "    ):\n",
    "        logger.info(\"Setting up Spider Dataset\")\n",
    "        return super().setup_dataset(\n",
    "            filter_by, num_rows, num_fewshot, model_name_or_path, prompt_template\n",
    "        )\n",
    "```\n",
    "\n",
    "Based on your requirements you can define all the necessary things in __init__ method and `setup_dataset` method. You can checkout `Text2SQLBaseDataset` class to see how things are defined. We will roll out a detailed tutorial on how to make a dataset for a different database very soon. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
